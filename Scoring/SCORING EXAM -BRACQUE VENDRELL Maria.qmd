---
title: "SCORING EXAM -BRACQUE VENDRELL Maria"
author: "Maria Bracque Vendrell"
format:
  html:
    #theme: darkly
    highlight: espresso
    code-copy: true
    code-fold: true
    df-print: paged
    number-sections: true
    toc: true
    toc_depth: 3
    toc_float: yes
execute: 
  #cache: true
  warning: false
editor: visual
fontsize: 11pt
---
```{r setup, include=FALSE} 
knitr::opts_chunk$set(message = FALSE) 
```

```{r, echo = FALSE, warning = FALSE, message = FALSE}

packages <- c("tidyverse", "ROCR", "car", "aod", "broom", "rsample", "bestglm", "glmnet", "glmnetUtils", "splines")
invisible(lapply(packages, library, character.only = TRUE))

additional_packages <- c( "purrr", "pROC", "foreign", "patchwork", "class", "scales", "rpart", "rpart.plot", "DescTools", "corrplot", "dplyr", "caret")
invisible(lapply(additional_packages, library, character.only = TRUE))

```

## Import data set

```{r}
default_dataset <- read.csv("default_dataset.csv")

```

## Model building and assessment

### Exploring the data set

```{r}
glimpse(default_dataset)

```

In this data set all variables are quantitative. TThere are 8971 companies.

#### Correlation

Let's check for correlated variables.

```{r}
corrplot(cor(default_dataset %>% select_if(is.double)), method="number")
```

We remove $cogs$ because it has high correlations with other variables and let's see how the table looks after.

```{r}

default_dataset_firststep <- subset(default_dataset, select = -(cogs))

corrplot(cor(default_dataset_firststep %>% select_if(is.double)), method="number")

```

We remove $ebit$ and $da$ because they have high correlations with $ebitda$ and let's see how the table looks after.

```{r}

default_dataset_firststep <- subset(default_dataset_firststep, select = -c(ebit,da))

corrplot(cor(default_dataset_firststep %>% select_if(is.double)), method="number")

```

We remove $ta$ because it has high correlations with other variables and let's see how the table looks after.

```{r}

default_dataset_firststep <- subset(default_dataset_firststep, select = -(ta))

corrplot(cor(default_dataset_firststep %>% select_if(is.double)), method="number")
```

We remove $nsale$ because it has high correlations with other variables and let's see how the table looks after.

```{r}

default_dataset_firststep <- subset(default_dataset_firststep, select = -(nsale))

corrplot(cor(default_dataset_firststep %>% select_if(is.double)), method="number")
```

We remove $opex$ because it has high correlations with other variables and let's see how the table looks after.

```{r}

default_dataset_firststep <- subset(default_dataset_firststep, select = -(opex))

corrplot(cor(default_dataset_firststep %>% select_if(is.double)), method="number")
```

We remove $tcl$ because it has high correlations with other variables and let's see how the table looks after.

```{r}

default_dataset_firststep <- subset(default_dataset_firststep, select = -(tcl))

corrplot(cor(default_dataset_firststep %>% select_if(is.double)), method="number")
```

We remove $gp$ because it has high correlations with other variables and let's see how the table looks after.

```{r}

default_dataset_firststep <- subset(default_dataset_firststep, select = -(gp))

corrplot(cor(default_dataset_firststep %>% select_if(is.double)), method="number")
```

We remove $tl$ because it has high correlations with other variables and let's see how the table looks after.

```{r}

default_dataset_firststep <- subset(default_dataset_firststep, select = -(tl))

corrplot(cor(default_dataset_firststep %>% select_if(is.double)), method="number")
```

There are no other correlations higher than 0.9, we can stop here deleting variables.

However, we can delete $default\_will\_occur$ and $fiscal\_year\_default$ since they are redundant.

```{r}

default_dataset_firststep <- subset(default_dataset_firststep, select = -c(default_will_occur, fiscal_year_default))

```

#### Descriptive Statistics

```{r}
summary(default_dataset_firststep %>% select_if(is.double))
```

With this table we can see the main statistics of the quantitative variables. For example, the lower $ebitda$ is -8218.50 whereas the maximum is 78669, with a global $ebitda$ mean of 343.1.

#### Interactions with the response variable

Let's plot the bivariate (scatter plots) relationships between the variable of interest default and the predictors:

```{r}

vars <- names(default_dataset_firststep %>% select_if(is.double))

for(var in vars){
    var <- as.name(var)
    print(ggplot(default_dataset_firststep %>%
                     mutate(Y = as.numeric(default)-1), aes(x=!! var,y = default)) +
        geom_jitter(height = 0.1, width = 0) +
        geom_smooth(method = "glm", 
                    formula = y ~ x,
                    method.args = list(family = "binomial"), 
                    se = FALSE,
                    col = "dodgerblue"))
}
```

### Model building

#### First step: Backward stepwise selection

```{r, warning = FALSE}
#define intercept-only model
intercept_only <- glm(default ~ 1, data=default_dataset, family="binomial")

#define model with all predictors
vars <- names(default_dataset %>% select_if(is.double))
all <- glm(default ~ ., data=default_dataset %>% select(default, vars) , family="binomial")
```

```{r, warning = FALSE}
first_step <- add1(intercept_only, scope = formula(all), test = "LRT")
first_step <- first_step %>% 
    tibble() %>%
    add_column(variable=row.names(first_step)) %>% 
    arrange(desc(LRT))
first_step
```

$mv$ is the most significant variable.

```{r, warning = FALSE}
second_step <- add1(update(intercept_only, ~. + mv), scope = formula(all), test = "LRT")
second_step <- second_step %>% 
    tibble() %>%
    add_column(variable=row.names(second_step)) %>% 
    arrange(desc(LRT))
second_step
```

$tltd$ is the most significant variable.

```{r, warning = FALSE}
third_step <- add1(update(intercept_only, ~. + mv + tltd), scope = formula(all), test = "LRT")
third_step <- third_step %>% 
    tibble() %>%
    add_column(variable=row.names(third_step)) %>% 
    arrange(desc(LRT))
third_step
```

$tcl$ is the most significant variable.

```{r, warning = FALSE}
fourth_step <- add1(update(intercept_only, ~. + mv + tltd + tcl), scope = formula(all), test = "LRT")
fourth_step <- fourth_step %>% 
    tibble() %>%
    add_column(variable=row.names(fourth_step)) %>% 
    arrange(desc(LRT))
fourth_step
```

$ni$ is the most significant variable.

```{r, warning = FALSE}
fifth_step <- add1(update(intercept_only, ~. + mv + tltd + tcl + ni), scope = formula(all), test = "LRT")
fifth_step <- fifth_step %>% 
    tibble() %>%
    add_column(variable=row.names(fifth_step)) %>% 
    arrange(desc(LRT))
fifth_step
```

$current\_asset$ is the most significant variable.

```{r, warning = FALSE}
sixth_step <- add1(update(intercept_only, ~. + mv + tltd + tcl + ni + current_asset), scope = formula(all), test = "LRT")
sixth_step <- sixth_step %>% 
    tibble() %>%
    add_column(variable=row.names(sixth_step)) %>% 
    arrange(desc(LRT))
sixth_step
```

$invt$ is the most significant variable.

```{r, warning = FALSE}
seventh_step <- add1(update(intercept_only, ~. + mv + tltd + tcl + ni + current_asset + invt), scope = formula(all), test = "LRT")
seventh_step <- seventh_step %>% 
    tibble() %>%
    add_column(variable=row.names(seventh_step)) %>% 
    arrange(desc(LRT))
seventh_step
```

Thus, the model is:

```{r, warning = FALSE}
model <- glm(default ~ mv + tltd + tcl + ni + current_asset + invt,
                   data = default_dataset,
                   family = binomial(link = "logit"))
model
```

Here, the predicted logit of defaulting for a company with zero market value, zero total long term debt, zero total current liabilities, zero net income, zero total value of current assets, and zero inventory or stocks is -0.447.

In addition, each one unit difference in $mv$/10000 (market value) is associated with a difference of 9.170 in the predicted logit of defaulting.

##### Test on model coefficients: Wald test

```{r}
car::Anova(model, type=3, test.statistic= "Wald")
aod::wald.test(b = coef(model), Sigma = vcov(model), Terms = 1:3)
```

The null hypothesis is rejected for all the coefficients.They are thus all different from 0.

##### Comparison of two models : Likelihood ratio tests

```{r}
my_model_1 <- model

full_model <- glm(default ~ cogs + current_asset + da + ebit + ebitda + gp + invt + mv + ni + nsale + opex + re + ta + tcl + tl + tltd + tr + trec, data = default_dataset)

anova(full_model, my_model_1, test = "LRT")

```

The difference in deviance between the two models is significant (p \< 0.05), suggesting that Model 2 provides a significantly better fit to the data than Model 1. Thus, we prefer $my\_model$ over $full\_model$.

#### Second step: Splitting the data set

```{r}
# Sort the data by the fiscal_year variable
sorted_data <- default_dataset %>% arrange(fiscal_year)

train <- filter(sorted_data , fiscal_year <= 2010)
test <- filter(sorted_data , fiscal_year > 2010)

```

##### Logistic Regression classifier chosen model

```{r}

# Create a Logistic Regression model on the training set
logistic_mymodel <- glm(default ~ mv + tltd + tcl + ni + current_asset + invt, data = train)
summary(logistic_mymodel)

# Predict class labels for the training set
train_probabilities_myglm <- predict(logistic_mymodel, newdata = train)
train_predictions_myglm <- ifelse(train_probabilities_myglm >= 0.5, 1, 0)

# Predict class labels for the test set
test_probabilities_myglm <- predict(logistic_mymodel, newdata = test)
test_predictions_myglm <- ifelse(test_probabilities_myglm >= 0.5, 1, 0)

# Calculate the error rates for both the training and test sets
train_error_rate_myglm <- mean(train_predictions_myglm != train$default)
train_error_rate_myglm

test_error_rate_myglm <- mean(test_predictions_myglm != test$default)
test_error_rate_myglm
```

##### Logistic Regression classifier full model

```{r, warning = FALSE}

# Create a Logistic Regression model on the training set
logistic_fullmodel <- glm(default ~ ., data = train)
summary(logistic_fullmodel)

# Predict class labels for the training set
train_probabilities_fullglm <- predict(logistic_fullmodel, newdata = train)
train_predictions_fullglm <- ifelse(train_probabilities_fullglm >= 0.5, 1, 0)

# Predict class labels for the test set
test_probabilities_fullglm <- predict(logistic_fullmodel, newdata = test)
test_predictions_fullglm <- ifelse(test_probabilities_fullglm >= 0.5, 1, 0)

# Calculate the error rates for both the training and test sets
train_error_rate_fullglm <- mean(train_predictions_fullglm != train$default)
train_error_rate_fullglm

test_error_rate_myglm <- mean(test_predictions_fullglm != test$default)
test_error_rate_myglm
```

##### ROC

```{r}
library("ROCR")

pred <- prediction(test_probabilities_myglm, test$default)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve my model", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")
abline(0, 1)

pred2 <- prediction(test_probabilities_fullglm, test$default)
perf2 <- performance(pred2, measure = "tpr", x.measure = "fpr")
plot(perf2, main="ROC curve full model", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)
```

#### Third Step

##### Method 2: Best Subset

```{r}
library(bestglm)

# For this method we can only have 15 initial variables, that is why I will use the default_dataset_firststep instead of the default_dataset

default_best <- as.data.frame(default_dataset_firststep %>% mutate(Y=if_else(default==1, 1, 0)) %>% select(-c(default, company_id, fiscal_year)))

my_model_2 <- bestglm(default_best)
my_model_2
```
Let's plot the ROC curve for $my\_model\_2$ using the method of the second step.

```{r}

# Create a Logistic Regression model on the training set
my_model_2 <- glm(default ~ mv , data = train)
logistic_mymodel2 <- my_model_2
summary(logistic_mymodel2)

# Predict class labels for the training set
train_probabilities_myglm2 <- predict(logistic_mymodel2, newdata = train)
train_predictions_myglm2 <- ifelse(train_probabilities_myglm2 >= 0.5, 1, 0)

# Predict class labels for the test set
test_probabilities_myglm2 <- predict(logistic_mymodel2, newdata = test)
test_predictions_myglm2 <- ifelse(test_probabilities_myglm2 >= 0.5, 1, 0)

# Calculate the error rates for both the training and test sets
train_error_rate_myglm2 <- mean(train_predictions_myglm2 != train$default)
train_error_rate_myglm2

test_error_rate_myglm2 <- mean(test_predictions_myglm2 != test$default)
test_error_rate_myglm2

pred_2 <- prediction(test_probabilities_myglm2, test$default)
perf_2 <- performance(pred_2, measure = "tpr", x.measure = "fpr")
plot(perf_2, main="ROC curve my_model_2", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)
```


##### Method 3: Penalized Lasso Regression

```{r}
library(glmnet)
library(glmnetUtils)

default_lasso <- glmnetUtils::glmnet(Y ~ ., data=default_best, family="binomial", alpha=1)

plot(default_lasso)
```
```{r}
lasso_result <- as_tibble(as.matrix(cbind(default_lasso$lambda, t(default_lasso$beta))))
names(lasso_result) <-  c("lambda", row.names(default_lasso$beta))
lasso_result
```
```{r}
default_lasso$terms
```

So $my\_model\_3$ will use the following variables: $current\_asset$, $ebitda$, $invt$, $mv$, $ni$, $re$, $tltd$, $tr$, and $trec$.

```{r}
my_model_3 <- glm(default ~ current_asset + ebitda + invt + mv + ni + re + tltd + tr + trec, data = train)
```

Let's plot the ROC curve for $my\_model\_3$ using the method of the second step.

```{r}

# Create a Logistic Regression model on the training set
logistic_mymodel3 <- my_model_3
summary(logistic_mymodel3)

# Predict class labels for the training set
train_probabilities_myglm3 <- predict(logistic_mymodel3, newdata = train)
train_predictions_myglm3 <- ifelse(train_probabilities_myglm3 >= 0.5, 1, 0)

# Predict class labels for the test set
test_probabilities_myglm3 <- predict(logistic_mymodel3, newdata = test)
test_predictions_myglm3 <- ifelse(test_probabilities_myglm3 >= 0.5, 1, 0)

# Calculate the error rates for both the training and test sets
train_error_rate_myglm3 <- mean(train_predictions_myglm3 != train$default)
train_error_rate_myglm3

test_error_rate_myglm3 <- mean(test_predictions_myglm3 != test$default)
test_error_rate_myglm3

pred_3 <- prediction(test_probabilities_myglm3, test$default)
perf_3 <- performance(pred_3, measure = "tpr", x.measure = "fpr")
plot(perf_3, main="ROC curve my_model_3", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)
```

Let's plot all the ROC curves on the same graph.

```{r}
library("ROCR")

plot(perf2, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)

plot(perf, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")

plot(perf_2, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "aquamarine3")

plot(perf_3, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "plum4")

legend(0.6,0.6,
       c('full_modell', 'my_model_1', 'my_model_2', 'my_model_3'),
       col=c("dodgerblue", "darkorange", "aquamarine3", "plum4"),lwd=3)
```
Here we can see that my_model_1 has the greatest AUC. 

## Model improvement and prediction using a validation set

### Feature engineering

#### First improvement

First, we are going to scale down the following variables by the total asset ($ta$): $re$, $ebit$, $mv$, $nsale$. The choice of these variables is driven by the Altman Z-score.

```{r}
default_5_1 <- default_dataset
default_5_1$re <- default_5_1$re/default_5_1$ta
default_5_1$ebit <- default_5_1$ebit/default_5_1$ta
default_5_1$mv <- default_5_1$mv/default_5_1$ta
default_5_1$nsale <- default_5_1$nsale/default_5_1$ta
```

Second, when running our previous regressions, the following warning messages appeared: "Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred". Winsorizing some variables might help to prevent this issue. 

```{r}
default_5_1 <- default_5_1 %>%
    mutate(across(cogs:trec, ~ DescTools::Winsorize(.x , probs = c(0.025, 0.975))))

default_5_1 <- as.data.frame(default_5_1 %>% mutate(Y=if_else(default==1, 1, 0)) %>% select(-c(default, company_id, fiscal_year_default, default_will_occur)))
```

Now, let's create a model using this transformed data. We will use Backward stepwise selection.

```{r}
#define intercept-only model
intercept_only <- glm(Y ~ 1, data=default_5_1, family="binomial")

#define model with all predictors
vars <- names(default_5_1 %>% select_if(is.double))
all <- glm(Y ~ ., data=default_5_1 %>% select(Y, vars) , family="binomial")
```

```{r}
first_step <- add1(intercept_only, scope = formula(all), test = "LRT")
first_step <- first_step %>% 
  tibble() %>%
  add_column(variable=row.names(first_step)) %>% 
  arrange(desc(LRT))
first_step
```

$ni$ is the most significant variable.

```{r}
second_step <- add1(update(intercept_only, ~. + ni), scope = formula(all), test = "LRT")
second_step <- second_step %>% 
  tibble() %>%
  add_column(variable=row.names(second_step)) %>% 
  arrange(desc(LRT))
second_step
```

$mv$ is the most significant variable.

```{r}
third_step <- add1(update(intercept_only, ~. + ni + mv), scope = formula(all), test = "LRT")
third_step <- third_step %>% 
  tibble() %>%
  add_column(variable=row.names(third_step)) %>% 
  arrange(desc(LRT))
third_step
```

$ebit$ is the most significant variable.

```{r}
fourth_step <- add1(update(intercept_only, ~. + ni + mv + ebit), scope = formula(all), test = "LRT")
fourth_step <- fourth_step %>% 
  tibble() %>%
  add_column(variable=row.names(fourth_step)) %>% 
  arrange(desc(LRT))
fourth_step
```

$nsale$ is the most significant variable.

```{r}
fifth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale), scope = formula(all), test = "LRT")
fifth_step <- fifth_step %>% 
  tibble() %>%
  add_column(variable=row.names(fifth_step)) %>% 
  arrange(desc(LRT))
fifth_step
```

$re$ is the most significant variable.

```{r}
sixth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re), scope = formula(all), test = "LRT")
sixth_step <- sixth_step %>% 
  tibble() %>%
  add_column(variable=row.names(sixth_step)) %>% 
  arrange(desc(LRT))
sixth_step
```

$trec$ is the most significant variable.

```{r}
seventh_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re + trec), scope = formula(all), test = "LRT")
seventh_step <- seventh_step %>% 
  tibble() %>%
  add_column(variable=row.names(seventh_step)) %>% 
  arrange(desc(LRT))
seventh_step
```
$tcl$ is the most significant variable.

```{r}
eighth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re + trec + tcl), scope = formula(all), test = "LRT")
eighth_step <- eighth_step %>% 
  tibble() %>%
  add_column(variable=row.names(eighth_step)) %>% 
  arrange(desc(LRT))
eighth_step
```

$current\_asset$ is the most significant variable.

```{r}
nineth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re + trec + tcl + current_asset), scope = formula(all), test = "LRT")
nineth_step <- nineth_step %>% 
  tibble() %>%
  add_column(variable=row.names(nineth_step)) %>% 
  arrange(desc(LRT))
nineth_step
```


Thus, the model is:
  
```{r}
my_improved_model_1 <- glm(Y ~ ni + mv + ebit + nsale + re + trec + tcl + current_asset,
                           data = default_5_1,
                           family = binomial(link = "logit"))
my_improved_model_1
```

```{r}
# Sort the data by the fiscal_year variable
sorted_data <- default_5_1 %>% arrange(fiscal_year)

train <- filter(sorted_data , fiscal_year <= 2010)
test <- filter(sorted_data , fiscal_year > 2010)

```


Let's plot the ROC curve for $my\_improved\_model\_1$ using the method of the second step.

```{r}

# Predict class labels for the training set
train_probabilities_myimprglm1 <- predict(my_improved_model_1, newdata = train)
train_predictions_myimprglm1 <- ifelse(train_probabilities_myimprglm1 >= 0.5, 1, 0)

# Predict class labels for the test set
test_probabilities_myimprglm1 <- predict(my_improved_model_1, newdata = test)
test_predictions_myimprglm1 <- ifelse(test_probabilities_myimprglm1 >= 0.5, 1, 0)

# Calculate the error rates for both the training and test sets
train_error_rate_myimprglm1 <- mean(train_predictions_myimprglm1 != train$Y)
train_error_rate_myimprglm1

test_error_rate_myimprglm1 <- mean(test_predictions_myimprglm1 != test$Y)
test_error_rate_myimprglm1

pred_impr_1 <- prediction(test_probabilities_myimprglm1, test$Y)
perf_impr_1 <- performance(pred_impr_1, measure = "tpr", x.measure = "fpr")
plot(perf_impr_1, main="ROC curve my_improved_model_1", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)
```

#### Second improvement

In the data set, there are a lot more of companies that did not default than companies that defaulted.Thus, here I am going to try to deal with this response imbalance by sampling differently the training set.

```{r}
table(default_dataset$default)
```

```{r}
set.seed(123)
undersampled_data <- downSample(default_dataset, y = factor(default_dataset$default))

# Check the class distribution after undersampling
table(undersampled_data$default)
```
Now, let's create a model using this undersampled data. We will use Backward stepwise selection.

```{r}
#define intercept-only model
intercept_only <- glm(default ~ 1, data=undersampled_data, family="binomial")

#define model with all predictors
vars <- names(undersampled_data %>% select_if(is.double))
all <- glm(default ~ ., data=undersampled_data %>% select(default, vars) , family="binomial")
```
Again, we have the following warning message: "Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred". We will thus also winsorize some variables 

```{r}
undersampled_data <- undersampled_data %>%
    mutate(across(cogs:trec, ~ DescTools::Winsorize(.x , probs = c(0.025, 0.975))))

undersampled_data <- as.data.frame(undersampled_data %>% mutate(Y=if_else(default==1, 1, 0)) %>% select(-c(default, company_id, fiscal_year_default, default_will_occur)))
```

Let's start the Backward stepwise selection again.

```{r}
#define intercept-only model
intercept_only <- glm(Y ~ 1, data=undersampled_data, family="binomial")

#define model with all predictors
vars <- names(undersampled_data %>% select_if(is.double))
all <- glm(Y ~ ., data=undersampled_data %>% select(Y, vars) , family="binomial")
```

```{r}
first_step <- add1(intercept_only, scope = formula(all), test = "LRT")
first_step <- first_step %>% 
  tibble() %>%
  add_column(variable=row.names(first_step)) %>% 
  arrange(desc(LRT))
first_step
```

$ni$ is the most significant variable.

```{r}
second_step <- add1(update(intercept_only, ~. + ni), scope = formula(all), test = "LRT")
second_step <- second_step %>% 
  tibble() %>%
  add_column(variable=row.names(second_step)) %>% 
  arrange(desc(LRT))
second_step
```

$mv$ is the most significant variable.

```{r}
third_step <- add1(update(intercept_only, ~. + ni + mv), scope = formula(all), test = "LRT")
third_step <- third_step %>% 
  tibble() %>%
  add_column(variable=row.names(third_step)) %>% 
  arrange(desc(LRT))
third_step
```

$tcl$ is the most significant variable.

```{r}
fourth_step <- add1(update(intercept_only, ~. + ni + mv + tcl), scope = formula(all), test = "LRT")
fourth_step <- fourth_step %>% 
  tibble() %>%
  add_column(variable=row.names(fourth_step)) %>% 
  arrange(desc(LRT))
fourth_step
```

$trec$ is the most significant variable.

```{r}
fifth_step <- add1(update(intercept_only, ~. + ni + mv + tcl + trec), scope = formula(all), test = "LRT")
fifth_step <- fifth_step %>% 
  tibble() %>%
  add_column(variable=row.names(fifth_step)) %>% 
  arrange(desc(LRT))
fifth_step
```

$current\_asset$ is the most significant variable.

```{r}
sixth_step <- add1(update(intercept_only, ~. + ni + mv + tcl + trec + current_asset), scope = formula(all), test = "LRT")
sixth_step <- sixth_step %>% 
  tibble() %>%
  add_column(variable=row.names(sixth_step)) %>% 
  arrange(desc(LRT))
sixth_step
```

$ebit$ is the most significant variable.

```{r}
seventh_step <- add1(update(intercept_only, ~. + ni + mv + tcl + trec + current_asset + ebit), scope = formula(all), test = "LRT")
seventh_step <- seventh_step %>% 
  tibble() %>%
  add_column(variable=row.names(seventh_step)) %>% 
  arrange(desc(LRT))
seventh_step
```
$gp$ is the most significant variable.

```{r}
eighth_step <- add1(update(intercept_only, ~. + ni + mv + tcl + trec + current_asset + ebit + gp), scope = formula(all), test = "LRT")
eighth_step <- eighth_step %>% 
  tibble() %>%
  add_column(variable=row.names(eighth_step)) %>% 
  arrange(desc(LRT))
eighth_step
```

Thus, the model is:
  
```{r}
my_improved_model_2 <- glm(Y ~ ni + mv + tcl + trec + current_asset + ebit + gp,
                           data = undersampled_data,
                           family = binomial(link = "logit"))
my_improved_model_2
```

```{r}
# Sort the data by the fiscal_year variable
sorted_data <- undersampled_data %>% arrange(fiscal_year)

set.seed(123) 
train_index <- sample(1:nrow(sorted_data), round(nrow(sorted_data) *0.7))

train <- filter(sorted_data , fiscal_year <= 2010)
test <- filter(sorted_data , fiscal_year > 2010)

```

Let's plot the ROC curve for $my\_improved\_model\_2$ using the method of the second step.

```{r}

# Predict class labels for the training set
train_probabilities_myimprglm2 <- predict(my_improved_model_2, newdata = train)
train_predictions_myimprglm2 <- ifelse(train_probabilities_myimprglm2 >= 0.5, 1, 0)

# Predict class labels for the test set
test_probabilities_myimprglm2 <- predict(my_improved_model_2, newdata = test)
test_predictions_myimprglm2 <- ifelse(test_probabilities_myimprglm2 >= 0.5, 1, 0)

# Calculate the error rates for both the training and test sets
train_error_rate_myimprglm2 <- mean(train_predictions_myimprglm2 != train$Y)
train_error_rate_myimprglm2

test_error_rate_myimprglm2 <- mean(test_predictions_myimprglm2 != test$Y)
test_error_rate_myimprglm2

pred_impr_2 <- prediction(test_probabilities_myimprglm2, test$Y)
perf_impr_2 <- performance(pred_impr_2, measure = "tpr", x.measure = "fpr")
plot(perf_impr_2, main="ROC curve my_improved_model_2", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)
```
#### Combining First and Second improvements



```{r}
set.seed(123)
default_5_2 <- downSample(default_dataset, y = factor(default_dataset$default))

default_5_2$re <- default_5_2$re/default_5_2$ta
default_5_2$ebit <- default_5_2$ebit/default_5_2$ta
default_5_2$mv <- default_5_2$mv/default_5_2$ta
default_5_2$nsale <- default_5_2$nsale/default_5_2$ta

default_5_2 <- default_5_2 %>%
    mutate(across(cogs:trec, ~ DescTools::Winsorize(.x , probs = c(0.025, 0.975))))

default_5_2 <- as.data.frame(default_5_2 %>% mutate(Y=if_else(default==1, 1, 0)) %>% select(-c(default, company_id, fiscal_year_default, default_will_occur)))
```

Now, let's create a model using this transformed data. We will use Backward stepwise selection.

```{r}
#define intercept-only model
intercept_only <- glm(Y ~ 1, data=default_5_2, family="binomial")

#define model with all predictors
vars <- names(default_5_2 %>% select_if(is.double))
all <- glm(Y ~ ., data=default_5_2 %>% select(Y, vars) , family="binomial")
```

```{r}
first_step <- add1(intercept_only, scope = formula(all), test = "LRT")
first_step <- first_step %>% 
  tibble() %>%
  add_column(variable=row.names(first_step)) %>% 
  arrange(desc(LRT))
first_step
```

$ni$ is the most significant variable.

```{r}
second_step <- add1(update(intercept_only, ~. + ni), scope = formula(all), test = "LRT")
second_step <- second_step %>% 
  tibble() %>%
  add_column(variable=row.names(second_step)) %>% 
  arrange(desc(LRT))
second_step
```

$mv$ is the most significant variable.

```{r}
third_step <- add1(update(intercept_only, ~. + ni + mv), scope = formula(all), test = "LRT")
third_step <- third_step %>% 
  tibble() %>%
  add_column(variable=row.names(third_step)) %>% 
  arrange(desc(LRT))
third_step
```

$ebit$ is the most significant variable.

```{r}
fourth_step <- add1(update(intercept_only, ~. + ni + mv + ebit), scope = formula(all), test = "LRT")
fourth_step <- fourth_step %>% 
  tibble() %>%
  add_column(variable=row.names(fourth_step)) %>% 
  arrange(desc(LRT))
fourth_step
```

$re$ is the most significant variable.

```{r}
fifth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + re), scope = formula(all), test = "LRT")
fifth_step <- fifth_step %>% 
  tibble() %>%
  add_column(variable=row.names(fifth_step)) %>% 
  arrange(desc(LRT))
fifth_step
```

$nsale$ is the most significant variable.

```{r}
sixth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re), scope = formula(all), test = "LRT")
sixth_step <- sixth_step %>% 
  tibble() %>%
  add_column(variable=row.names(sixth_step)) %>% 
  arrange(desc(LRT))
sixth_step
```

$trec$ is the most significant variable.

```{r}
seventh_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re + trec), scope = formula(all), test = "LRT")
seventh_step <- seventh_step %>% 
  tibble() %>%
  add_column(variable=row.names(seventh_step)) %>% 
  arrange(desc(LRT))
seventh_step
```
$tcl$ is the most significant variable.

```{r}
eighth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re + trec + tcl), scope = formula(all), test = "LRT")
eighth_step <- eighth_step %>% 
  tibble() %>%
  add_column(variable=row.names(eighth_step)) %>% 
  arrange(desc(LRT))
eighth_step
```

$current\_asset$ is the most significant variable.

```{r}
nineth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re + trec + tcl + current_asset), scope = formula(all), test = "LRT")
nineth_step <- nineth_step %>% 
  tibble() %>%
  add_column(variable=row.names(nineth_step)) %>% 
  arrange(desc(LRT))
nineth_step
```
$ebitda$ is the most significant variable.

```{r}
tenth_step <- add1(update(intercept_only, ~. + ni + mv + ebit + nsale + re + trec + tcl + current_asset + ebitda), scope = formula(all), test = "LRT")
tenth_step <- tenth_step %>% 
  tibble() %>%
  add_column(variable=row.names(tenth_step)) %>% 
  arrange(desc(LRT))
tenth_step
```

Thus, the model is:
  
```{r}
my_improved_model_3 <- glm(Y ~ ni + mv + ebit + nsale + re + trec + tcl + current_asset + ebitda,
                           data = default_5_2,
                           family = binomial(link = "logit"))
my_improved_model_3
```

```{r}
# Sort the data by the fiscal_year variable
sorted_data <- default_5_2 %>% arrange(fiscal_year)

train <- filter(sorted_data , fiscal_year <= 2010)
test <- filter(sorted_data , fiscal_year > 2010)

```


Let's plot the ROC curve for $my\_improved\_model\_3$ using the method of the second step.

```{r}

# Predict class labels for the training set
train_probabilities_myimprglm3 <- predict(my_improved_model_3, newdata = train)
train_predictions_myimprglm3 <- ifelse(train_probabilities_myimprglm3 >= 0.5, 1, 0)

# Predict class labels for the test set
test_probabilities_myimprglm3 <- predict(my_improved_model_3, newdata = test)
test_predictions_myimprglm3 <- ifelse(test_probabilities_myimprglm3 >= 0.5, 1, 0)

# Calculate the error rates for both the training and test sets
train_error_rate_myimprglm3 <- mean(train_predictions_myimprglm3 != train$Y)
train_error_rate_myimprglm3

test_error_rate_myimprglm3 <- mean(test_predictions_myimprglm3 != test$Y)
test_error_rate_myimprglm3

pred_impr_3 <- prediction(test_probabilities_myimprglm3, test$Y)
perf_impr_3 <- performance(pred_impr_3, measure = "tpr", x.measure = "fpr")
plot(perf_impr_3, main="ROC curve my_improved_model_3", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)

```
Let's plot all the ROC curves of the improved models on the same graph.

```{r}
plot(perf_impr_1, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)

plot(perf_impr_2, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")

plot(perf_impr_3, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "aquamarine3")


legend(0.6,0.6,
       c('my_improved_model_1', 'my_improved_model_2', 'my_improved_model_3'),
       col=c("dodgerblue", "darkorange", "aquamarine3"),lwd=3)


```

### Resampling validation

Classic K-fold cross-validation may not be adapted to this data set due to the presence of temporal dependencies. Indeed, K-fold cross-validation randomly splits the data into training and testing sets, potentially breaking the temporal dependencies among the years, and this can lead to overly optimistic performance estimates.

```{r}
# Sort the data by the fiscal_year variable
sorted_data_resamplig <- as.data.frame(default_dataset %>% arrange(fiscal_year))

# Number of splits
n_splits <- 5

# Number of training and testing periods the first iteration
n_train_periods <- 2

# Initialize a vector to store AUC values for each split
auc_values <- numeric()

# Iterate through the time series
for (i in 1:n_splits+1) {
  # Split data into training and testing sets
  train_data <- filter(sorted_data_resamplig , fiscal_year <= 2000 + (i + n_train_periods - 1))
  test_data <- filter(sorted_data_resamplig , fiscal_year > 2000 + (i + n_train_periods - 1))
  n_train_periods <- n_train_periods + 1

  # I am choosing the variables of my_improved_model_2 since it has the best results to build the model
  model <- glm(default ~ ni + mv + ebit + nsale + re + trec + tcl + current_asset + ebitda,
               data = train_data, family = binomial) 
  
  # Compute AUC for the training set
  pred_train <- ROCR::prediction(model$fitted.values, train_data$default)
  perf_train <- ROCR::performance(pred_train, measure = "tpr", x.measure = "fpr")

  auc_train <- ROCR::performance(pred_train, measure = "auc")
  auc_train <- auc_train@y.values[[1]]
  
  test_predict <- predict(model, newdata=test_data, type="response")

  pred_test <- ROCR::prediction(test_predict, test_data$default)
  perf_test <- ROCR::performance(pred_test, measure = "tpr", x.measure = "fpr")
  
  
  # Store the AUC value
  auc_values <- c(auc_values, auc_train)
}

auc_values

```
We can see that the AUC increases with the number of years in the training set.


Let's plot all the ROC curves of the project on the same graph.

```{r}

plot(perf2, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")
abline(0, 1)

plot(perf, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")

plot(perf_2, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "aquamarine3")

plot(perf_3, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "plum4")

plot(perf_impr_1, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue4")
abline(0, 1)

plot(perf_impr_2, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange4")

plot(perf_impr_3, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "aquamarine4")


plot(perf_test, add = TRUE, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "red3")

legend(0.6,0.6,
       c('full_modell', 'my_model_1', 'my_model_2', 'my_model_3', 'my_improved_model_1', 'my_improved_model_2', 'my_improved_model_3', "resampling_model"),
       col=c("dodgerblue", "darkorange", "aquamarine3", "plum4", "dodgerblue4", "darkorange4", "aquamarine4", "red3" ),lwd=3)


```

Within my_models, the best model is my_model_1 is the best one. Within my improved models, the best one is my_improved_model_2.

```{r}

my_best_model <- my_model_1
my_best_improved_model <- my_improved_model_2

```

### Prediction on validation test / final model selection

```{r}
default_dataset_validation <- read_csv('default_dataset_validation.csv')
glimpse(default_dataset_validation)
```
#### Validation set with my_best_model

```{r}
# use fitted model to predict value on testing set
test_predict_bm <- predict(my_best_model, newdata=default_dataset_validation, type="response")

# plot ROC / compute AUC for the training set
pred_test_bm <- ROCR::prediction(test_predict_bm, default_dataset_validation$default)
perf_test_bm <- ROCR::performance(pred_test_bm, measure = "tpr", x.measure = "fpr")
plot(perf_test_bm, main="ROC curve my_best_model", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")

auc_test_bm <- ROCR::performance(pred_test_bm, measure = "auc")
auc_test_bm <- auc_test_bm@y.values[[1]]
auc_test_bm
```

#### Validation set with my_improved_best_model

```{r}
# use fitted model to predict value on testing set
test_predict_bim <- predict(my_best_improved_model, newdata=default_dataset_validation, type="response")

# plot ROC / compute AUC for the training set
pred_test_bim <- ROCR::prediction(test_predict_bim, default_dataset_validation$default)
perf_test_bim <- ROCR::performance(pred_test_bim, measure = "tpr", x.measure = "fpr")
plot(perf_test_bim, main="ROC curve my_best_improved_model", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")

auc_test_bim <- ROCR::performance(pred_test_bim, measure = "auc")
auc_test_bim <- auc_test_bim@y.values[[1]]
auc_test_bim
```

#### Validation set with full_model
```{r}
# use fitted model to predict value on testing set
test_predict_fm <- predict(full_model, newdata=default_dataset_validation, type="response")

# plot ROC / compute AUC for the training set
pred_test_fm <- ROCR::prediction(test_predict_fm, default_dataset_validation$default)
perf_test_fm <- ROCR::performance(pred_test_fm, measure = "tpr", x.measure = "fpr")
plot(perf_test_fm, main="ROC curve my_full_model", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")

auc_test_fm <- ROCR::performance(pred_test_fm, measure = "auc")
auc_test_fm <- auc_test_fm@y.values[[1]]
auc_test_fm
```

Let's plot the three ROC curves on the same graph.

```{r}
plot(perf_test_bm, main="ROC curves", xlab="Specificity",
     ylab="Sensitivity", col = "dodgerblue")

plot(perf_test_bim, add = TRUE, main="ROC curve ", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")

plot(perf_test_fm, add = TRUE, main="ROC curve ", xlab="Specificity",
     ylab="Sensitivity", col = "aquamarine3")

legend(0.6,0.6,
       c('my_best_model', 'my_best_improved_model', 'my_full_model'),
       col=c("dodgerblue", "darkorange", "aquamarine3"),lwd=3)
```

Thus, to sum up, the model with the bests results is my_best_model: glm(formula = default ~ mv + tltd + tcl + ni + current_asset + invt, family = binomial(link = "logit"), data = default_dataset). It is the first model obtained by Backward Stepwise selection. 